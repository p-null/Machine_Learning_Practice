{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA for news content",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MQZ2DF6ayORu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "outputId": "cc132d54-04fa-4a97-9b93-ebed20d717a7"
      },
      "cell_type": "code",
      "source": [
        "# Run in terminal or command prompt\n",
        "!pip install spacy\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.18)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Collecting numpy>=1.15.0 (from spacy)\n",
            "  Using cached https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (4.28.1)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy) (0.4.3.2)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy) (0.9.0)\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "85rmE9iYsDuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97790c22-cd01-479f-c3ef-f1e19ec1848e"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "#import pyLDAvis\n",
        "#import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uaHE9q2uyvPE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare NLTK stop words"
      ]
    },
    {
      "metadata": {
        "id": "f2KmIK4fsFDW",
        "colab_type": "code",
        "outputId": "7dc89cd3-5808-4fc3-96f1-003a2907b813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'The New York Times'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MI64CLUUex3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "80cf6f4d-6b3c-43cc-a181-1834d76185cc"
      },
      "cell_type": "code",
      "source": [
        "!head -n 1 news.csv"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e compounds are, in turn, the basic energy supply for almost all animal cells, including those of humans. The mathematical description of photosynthesis is sometimes billed as “the equation that powers the world. ” For a decade, Dr. Long had argued that photosynthesis was not actually very efficient. In the course of evolution, several experts said, Mother Nature had focused on the survival and reproduction of plants, not on putting out the maximum amount of seeds or fruits for humans to come along and pick. Dr. Long thought crop yields might be improved by certain genetic changes. Other scientists doubted it would work, but with the Science paper, Dr. Long and his collaborator  —   Krishna K. Niyogi, who holds appointments at the University of California, Berkeley, and the Lawrence Berkeley National Laboratory  —   have gone a long way toward proving their point. Much of the work at the University of Illinois was carried out by two young researchers from abroad who hold positions in Dr. Long’s laboratory, Johannes Kromdijk of the Netherlands and Katarzyna Glowacka of Poland. No one plans to eat tobacco, of course, nor does the Gates Foundation have any interest in increasing the production of that   crop. But the researchers used it because tobacco is a particularly fast and easy plant in which to try new genetic alterations to see how well they work. In a recent interview here, Dr. Kromdijk and Dr. Glowacka showed off tiny tobacco plants incorporating the genetic changes and described their aspirations. “We hope it translates into food crops in the way we’ve shown in tobacco,” Dr. Kromdijk said. “Of course, you only know when you actually try it. ” In the initial work, the researchers transferred genes from a common laboratory plant, known as thale cress or   cress, into strains of tobacco. The effect was not to introduce alien substances, but rather to increase the level of certain proteins that already existed in tobacco. When plants receive direct sunlight, they are often getting more energy than they can use, and they activate a mechanism that helps them shed it as heat  —   while slowing carbohydrate production. The genetic changes the researchers introduced help the plant turn that mechanism off faster once the excessive sunlight ends, so that the machinery of photosynthesis can get back more quickly to maximal production of carbohydrates. It is a bit like a factory worker taking a shorter coffee break before getting back to the assembly line. But the effect on the overall growth of the tobacco plants was surprisingly large. When the scientists grew the newly created plants in fields at the University of Illinois, they achieved yield increases of 13. 5 percent in one strain, 19 percent in a second and 20 percent in a third, over normal tobacco plants grown for comparison. Because the machinery of photosynthesis in many of the world’s food crops is identical to that of tobacco, theory suggests that a comparable manipulation of those crops should increase production. Work is planned to test that in crops that are especially important as dietary staples in Africa, like cowpeas, rice and cassava. Two outside experts not involved in the research both used the word “exciting” to describe it. But they emphasized that the researchers had not yet proved that the food supply could be increased. “How does it look in rice or corn or wheat or sugar beets?” said L. Val Giddings, a senior fellow at the Information Technology and Innovation Foundation in Washington and a longtime advocate of   crops. “You’ve got to get it into a handful of the important crops before you can show this is real and it’s going to have a huge impact. We are not there yet. ” Barry D. Bruce of the University of Tennessee at Knoxville, who studies photosynthesis, pointed out that the genetic alteration might behave differently in crops where only parts of the plant, such as seeds or fruits, are harvested. In tobacco, by contrast, the entire aboveground plant is harvested  —   Dr. Bruce called it “a leafy green plant used for cigars!” Dr. Bruce also noted that, now that the principle has been established, it might be possible to find plant varieties with the desired traits and introduce the changes into crops by conventional breeding, rather than by genetic engineering. Dr. Long and his group agreed this might be possible. The genetic engineering approach, if it works, may well be used in commercial seeds produced by Western agricultural companies. One of them, Syngenta, has already signed a deal to get a first look at the results. But the Gates Foundation is determined to see the technology, assuming its early promise is borne out, make its way to African farmers at low cost. The work is, in part, an effort to secure the food supply against the possible effects of future climate change. If rising global temperatures cut the production of food, human society could be destabilized, but more efficient crop plants could potentially make the food system more resilient, Dr. Long said. “We’re in a year when commodity prices are very low, and people are saying the world doesn’t need more food,” Dr. Long said. “But if we don’t do this now, we may not have it when we really need it. ”\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zScXNhD4ymRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "fc5a416c-2279-4ff9-88e2-6e7898f3c996"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('news.csv', header=0)\n",
        "# Convert to list\n",
        "data = df.content.values.tolist()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3bc97d999bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'news.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Convert to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'content'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3a4SF6Roe2Ry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1084
        },
        "outputId": "29097942-3ddd-4d86-e20f-be23a2938d9b"
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>e compounds are</th>\n",
              "      <th>in turn</th>\n",
              "      <th>the basic energy supply for almost all animal cells</th>\n",
              "      <th>including those of humans. The mathematical description of photosynthesis is sometimes billed as “the equation that powers the world. ” For a decade</th>\n",
              "      <th>Dr. Long had argued that photosynthesis was not actually very efficient. In the course of evolution</th>\n",
              "      <th>several experts said</th>\n",
              "      <th>Mother Nature had focused on the survival and reproduction of plants</th>\n",
              "      <th>not on putting out the maximum amount of seeds or fruits for humans to come along and pick. Dr. Long thought crop yields might be improved by certain genetic changes. Other scientists doubted it would work</th>\n",
              "      <th>but with the Science paper</th>\n",
              "      <th>Dr. Long and his collaborator  —   Krishna K. Niyogi</th>\n",
              "      <th>...</th>\n",
              "      <th>assuming its early promise is borne out</th>\n",
              "      <th>make its way to African farmers at low cost. The work is</th>\n",
              "      <th>in part</th>\n",
              "      <th>an effort to secure the food supply against the possible effects of future climate change. If rising global temperatures cut the production of food</th>\n",
              "      <th>human society could be destabilized</th>\n",
              "      <th>but more efficient crop plants could potentially make the food system more resilient</th>\n",
              "      <th>Dr. Long said. “We’re in a year when commodity prices are very low</th>\n",
              "      <th>and people are saying the world doesn’t need more food</th>\n",
              "      <th>” Dr. Long said. “But if we don’t do this now</th>\n",
              "      <th>we may not have it when we really need it. ”\"</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4605</td>\n",
              "      <td>22384</td>\n",
              "      <td>Where to Be Single in New York - The New York ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Kim Velsey</td>\n",
              "      <td>2017-03-31</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There may be no such thing as an ideal neighbo...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4606</td>\n",
              "      <td>22385</td>\n",
              "      <td>When Eve and Eve Bit the Apple - The New York ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Kristen Scharold</td>\n",
              "      <td>2017-04-06</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When you are raised to be a good Christian gir...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4607</td>\n",
              "      <td>22386</td>\n",
              "      <td>Trump Turns to His Right Flank to Fill Nationa...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Julie Hirschfeld Davis</td>\n",
              "      <td>2017-04-29</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —     Donald J. Trump moved quickl...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4608</td>\n",
              "      <td>22387</td>\n",
              "      <td>Ivanka Trump’s Presence at Meeting With Japan’...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Eric Lipton</td>\n",
              "      <td>2017-04-19</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —   The potential for conflicts of...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4609</td>\n",
              "      <td>22388</td>\n",
              "      <td>How Could You? 19 Questions to Ask Loved Ones ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Michael Barbaro</td>\n",
              "      <td>2016-11-20</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Here’s what we learned after the ugliest presi...</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 56 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   e compounds are   in turn  \\\n",
              "0             4605     22384   \n",
              "1             4606     22385   \n",
              "2             4607     22386   \n",
              "3             4608     22387   \n",
              "4             4609     22388   \n",
              "\n",
              "   the basic energy supply for almost all animal cells  \\\n",
              "0  Where to Be Single in New York - The New York ...     \n",
              "1  When Eve and Eve Bit the Apple - The New York ...     \n",
              "2  Trump Turns to His Right Flank to Fill Nationa...     \n",
              "3  Ivanka Trump’s Presence at Meeting With Japan’...     \n",
              "4  How Could You? 19 Questions to Ask Loved Ones ...     \n",
              "\n",
              "   including those of humans. The mathematical description of photosynthesis is sometimes billed as “the equation that powers the world. ” For a decade  \\\n",
              "0                                     New York Times                                                                                                      \n",
              "1                                     New York Times                                                                                                      \n",
              "2                                     New York Times                                                                                                      \n",
              "3                                     New York Times                                                                                                      \n",
              "4                                     New York Times                                                                                                      \n",
              "\n",
              "   Dr. Long had argued that photosynthesis was not actually very efficient. In the course of evolution  \\\n",
              "0                                         Kim Velsey                                                     \n",
              "1                                   Kristen Scharold                                                     \n",
              "2                             Julie Hirschfeld Davis                                                     \n",
              "3                                        Eric Lipton                                                     \n",
              "4                                    Michael Barbaro                                                     \n",
              "\n",
              "   several experts said  \\\n",
              "0            2017-03-31   \n",
              "1            2017-04-06   \n",
              "2            2017-04-29   \n",
              "3            2017-04-19   \n",
              "4            2016-11-20   \n",
              "\n",
              "    Mother Nature had focused on the survival and reproduction of plants  \\\n",
              "0                                             2017.0                       \n",
              "1                                             2017.0                       \n",
              "2                                             2017.0                       \n",
              "3                                             2017.0                       \n",
              "4                                             2016.0                       \n",
              "\n",
              "    not on putting out the maximum amount of seeds or fruits for humans to come along and pick. Dr. Long thought crop yields might be improved by certain genetic changes. Other scientists doubted it would work  \\\n",
              "0                                                3.0                                                                                                                                                                \n",
              "1                                                4.0                                                                                                                                                                \n",
              "2                                                4.0                                                                                                                                                                \n",
              "3                                                4.0                                                                                                                                                                \n",
              "4                                               11.0                                                                                                                                                                \n",
              "\n",
              "    but with the Science paper  \\\n",
              "0                          NaN   \n",
              "1                          NaN   \n",
              "2                          NaN   \n",
              "3                          NaN   \n",
              "4                          NaN   \n",
              "\n",
              "   Dr. Long and his collaborator  —   Krishna K. Niyogi  \\\n",
              "0  There may be no such thing as an ideal neighbo...      \n",
              "1  When you are raised to be a good Christian gir...      \n",
              "2  WASHINGTON  —     Donald J. Trump moved quickl...      \n",
              "3  WASHINGTON  —   The potential for conflicts of...      \n",
              "4  Here’s what we learned after the ugliest presi...      \n",
              "\n",
              "                        ...                        \\\n",
              "0                       ...                         \n",
              "1                       ...                         \n",
              "2                       ...                         \n",
              "3                       ...                         \n",
              "4                       ...                         \n",
              "\n",
              "    assuming its early promise is borne out  \\\n",
              "0                                       NaN   \n",
              "1                                       NaN   \n",
              "2                                       NaN   \n",
              "3                                       NaN   \n",
              "4                                       NaN   \n",
              "\n",
              "    make its way to African farmers at low cost. The work is   in part  \\\n",
              "0                                                NaN               NaN   \n",
              "1                                                NaN               NaN   \n",
              "2                                                NaN               NaN   \n",
              "3                                                NaN               NaN   \n",
              "4                                                NaN               NaN   \n",
              "\n",
              "    an effort to secure the food supply against the possible effects of future climate change. If rising global temperatures cut the production of food  \\\n",
              "0                                                NaN                                                                                                      \n",
              "1                                                NaN                                                                                                      \n",
              "2                                                NaN                                                                                                      \n",
              "3                                                NaN                                                                                                      \n",
              "4                                                NaN                                                                                                      \n",
              "\n",
              "    human society could be destabilized  \\\n",
              "0                                   NaN   \n",
              "1                                   NaN   \n",
              "2                                   NaN   \n",
              "3                                   NaN   \n",
              "4                                   NaN   \n",
              "\n",
              "    but more efficient crop plants could potentially make the food system more resilient  \\\n",
              "0                                                NaN                                       \n",
              "1                                                NaN                                       \n",
              "2                                                NaN                                       \n",
              "3                                                NaN                                       \n",
              "4                                                NaN                                       \n",
              "\n",
              "    Dr. Long said. “We’re in a year when commodity prices are very low  \\\n",
              "0                                                NaN                     \n",
              "1                                                NaN                     \n",
              "2                                                NaN                     \n",
              "3                                                NaN                     \n",
              "4                                                NaN                     \n",
              "\n",
              "    and people are saying the world doesn’t need more food  \\\n",
              "0                                                NaN         \n",
              "1                                                NaN         \n",
              "2                                                NaN         \n",
              "3                                                NaN         \n",
              "4                                                NaN         \n",
              "\n",
              "   ” Dr. Long said. “But if we don’t do this now  \\\n",
              "0                                            NaN   \n",
              "1                                            NaN   \n",
              "2                                            NaN   \n",
              "3                                            NaN   \n",
              "4                                            NaN   \n",
              "\n",
              "    we may not have it when we really need it. ”\"  \n",
              "0                                             NaN  \n",
              "1                                             NaN  \n",
              "2                                             NaN  \n",
              "3                                             NaN  \n",
              "4                                             NaN  \n",
              "\n",
              "[5 rows x 56 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "6f58LkYOyX5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.title[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J7TWMXv-zj9b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vZNQQPAuy5B5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "# Remove after observing a lot of new york times\n",
        "data = [re.sub(\"New York Times\", \"\", sent) for sent in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QIr296unzIs_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1MBcw9eBzI_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tokenize words and Clean-up text"
      ]
    },
    {
      "metadata": {
        "id": "JULcdAJAzdFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r86j9pgy3Yqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WuYA77et3ZGt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating Bigram Model"
      ]
    },
    {
      "metadata": {
        "id": "Gn1p52ic3ZFv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "pNbcaXq91xza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewe\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xd8q4Vl3gyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Remove Stopwords, Make Bigrams and Lemmatize"
      ]
    },
    {
      "metadata": {
        "id": "137wfarE3gOE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define functions for stopwords, bigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZkRHInQ3j-3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vI8k6qE23rdM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create the Dictionary and Corpus for Topic Modeling"
      ]
    },
    {
      "metadata": {
        "id": "etcEBlHX3nqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hik6eD7Z4HxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nemja6j-39gd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency)."
      ]
    },
    {
      "metadata": {
        "id": "ocZdWKz_4Mi3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the Topic Model"
      ]
    },
    {
      "metadata": {
        "id": "1nms2tPH4Lqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOqXlFch4RcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# View the topics in LDA model\n",
        "\n",
        "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "metadata": {
        "id": "bqXeAWCa3-Zw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "99ddQISM4Y4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip && unzip mallet-2.0.8.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EOrCqsh45SNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pAqI3__boYlI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Building LDA Mallet Model\n",
        "\n",
        "Gensim provides a wrapper to implement Mallet’s LDA from within Gensim itself. "
      ]
    },
    {
      "metadata": {
        "id": "tvF2kJi85ODL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mallet_path = 'mallet-2.0.8/bin/mallet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mErhBkeI51aG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzw-Q-Ec53yf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pprint(ldamallet.show_topics(formatted=False))\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_ldamallet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vgPjY4I4saF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # Find the optimal number of topics for LDA\n",
        " \n",
        " My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.\n",
        "\n",
        "Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.\n",
        "\n",
        "If  the same keywords is repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\n",
        "\n",
        "The compute_coherence_values()  trains multiple LDA models and provides the models and their corresponding coherence scores."
      ]
    },
    {
      "metadata": {
        "id": "r_jJcDd94twL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d241bBIP49Cv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=25, limit=60, step=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LrEtscM55DF9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show graph\n",
        "limit=60; start=25; step=10;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QQalTbc58PsZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZJtlG1c7fbM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the model and print the topics\n",
        "optimal_model = model_list[4]\n",
        "model_topics = optimal_model.show_topics(formatted=False)\n",
        "pprint(optimal_model.print_topics(num_words=10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Az2BZIcSpzXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7WsH4vIap0WY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Explain what can you find from the results\n",
        "\n",
        "Some of the topic can be easily found\n",
        "\n",
        "No.24: US election\n",
        "\n",
        "No.12: politics\n",
        "\n",
        "No.1: Military\n",
        "\n",
        "We can also see some of topics are overlaped."
      ]
    },
    {
      "metadata": {
        "id": "Dnwl55L08cR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Finding the dominant topic in each sentence\n",
        "\n",
        "To find that, we find the topic number that has the highest percentage contribution in that document."
      ]
    },
    {
      "metadata": {
        "id": "bk6jud2s8Xsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "df_dominant_topic.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5l-mTYvP4Zz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that for each sentence, it actually has keywords that conrespond to relevant topics. "
      ]
    },
    {
      "metadata": {
        "id": "lHi8c3Sp8hff",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Find the most representative document for each topic\n",
        "\n",
        "Find the documents a given topic has contributed to the most and infer the topic by reading that document."
      ]
    },
    {
      "metadata": {
        "id": "Qgu-pLN28h95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Group top 5 sentences under each topic\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index    \n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D6GUFY8e2dA6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each row above corresponds to one topic. It has the topic number, the keywords, and the most representative document. The Perc_Contribution column is nothing but the percentage contribution of the topic in the given document."
      ]
    },
    {
      "metadata": {
        "id": "jMKbg1678oXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Topic distribution across documents\n",
        "\n",
        "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
      ]
    },
    {
      "metadata": {
        "id": "mF04E4Nv8o8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of Documents for Each Topic\n",
        "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
        "\n",
        "# Percentage of Documents for Each Topic\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
        "\n",
        "# Topic Number and Keywords\n",
        "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
        "\n",
        "# Concatenate Column wise\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
        "\n",
        "# Change Column names\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
        "\n",
        "# Show\n",
        "df_dominant_topics[:29]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Vp4X4uL1cAi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the percentage of documents for each topic at the last column. \n",
        "\n",
        "Most of them cover the range between 2% to 4%"
      ]
    }
  ]
}